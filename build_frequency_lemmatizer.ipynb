{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is designed to scan the entire Tesserae text folder and build frequency data\n",
    "for each lemma in the corpus. This is necessary for certain experiments involving the ideal\n",
    "lemma (in ambiguous cases), synonym, or translation for a given token-in-context during\n",
    "NLP tasks. Contributors: James Gawley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, expanduser\n",
    "from collections import defaultdict\n",
    "from pprint import PrettyPrinter\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from operator import itemgetter\n",
    "from difflib import SequenceMatcher\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = os.path.join('~/cltk')\n",
    "path = os.path.expanduser(rel_path)\n",
    "os.chdir(path)\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.semantics.latin.lookup import Lemmata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-acbf46d148aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#This filepath needs to be customized. The git repo is located at https://github.com/jeffkinnison/tesserae-v5.git\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/tesserae-v5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtesserae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTessFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#This filepath needs to be customized. The git repo is located at https://github.com/jeffkinnison/tesserae-v5.git\n",
    "rel_path = os.path.join('~/tesserae-v5')\n",
    "path = os.path.expanduser(rel_path)\n",
    "os.chdir(path)\n",
    "from tesserae.utils import TessFile\n",
    "from tesserae.utils import TessFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COUNT_LIBRARY is a large dictionary whose keys are lemmas and whose values are intergers representing the number of times that lemma (may have) appeared in the training corpus.\n",
    "\n",
    "To build this data structure, the method read_files() moves through a .tess formated file, and extract words in situ as tokens, one at a time. When a given token has only one possible lemmatization, then that lemma's entry in COUNT_DICTIONARY is incremented. The interesting case comes when a token has two possible lemmas.\n",
    "\n",
    "In cases of ambiguous lemmatization, the COUNT_DICTIONARY entries for each possible lemma are incremented. This means that true positives and false positives are lumped in together. However over time, the true positives seem to outweigh the false positives, because this data structure can be built into a fairly accurate lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT_LIBRARY = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filepath):\n",
    "    '''Moves through a .tess file and calls the 'next' and 'count_lemma' functions as needed.\n",
    "    Updates the SKIP_LIBRARY global object.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath: a file in .tess format\n",
    "    '''\n",
    "    tessobj = TessFile(filepath)\n",
    "    tokengenerator = iter(tessobj.read_tokens())\n",
    "    stop = 0\n",
    "    while stop != 1:\n",
    "        try: \n",
    "            rawtoken = next(tokengenerator)\n",
    "            cleantoken_list = token_cleanup(rawtoken)\n",
    "            count_lemma(cleantoken_list[0])\n",
    "        except StopIteration:\n",
    "            stop = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Lemmata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1bd2c60ea558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLemmata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lemmata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'latin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_lemma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargettoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     '''Builds a complex data structure that will contain the 'average context'\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mparam\u001b[0m \u001b[0mtargettoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Lemmata' is not defined"
     ]
    }
   ],
   "source": [
    "lemmatizer = Lemmata(dictionary = 'lemmata', language = 'latin')\n",
    "def count_lemma(targettoken):\n",
    "    '''Builds a complex data structure that will contain the 'average context'\n",
    "    for each type in the corpus.\n",
    "    param targettoken: the token in question\n",
    "    param c: the context tokens\n",
    "    global SKIP_LIBRARY: a dictionary whose keys are types and whose values are\n",
    "    dictionaries; in turn their keys are context types and values are\n",
    "    incremented counts.\n",
    "    '''\n",
    "    global COUNT_LIBRARY\n",
    "    lemmas = lemmatizer.lookup([targettoken])\n",
    "    lemmas = lemmatizer.isolate(lemmas)\n",
    "    for lemma in lemmas:\n",
    "        if lemma not in COUNT_LIBRARY:\n",
    "            COUNT_LIBRARY[lemma] = 0\n",
    "        COUNT_LIBRARY[lemma] = COUNT_LIBRARY[lemma] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jv = JVReplacer()\n",
    "word_tokenizer = WordTokenizer('latin')\n",
    "def token_cleanup(rawtoken):\n",
    "    # this cleaning algorithm is a potential area for improvement.\n",
    "    rawtoken = jv.replace(rawtoken)\n",
    "    rawtoken = rawtoken.lower()\n",
    "    tokenlist = word_tokenizer.tokenize(rawtoken)\n",
    "    #sometimes words are split into enclitics and punctuation.\n",
    "    return tokenlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the actual program loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open all the tesserae files\n",
    "relativepath = join('~/cleantess/tesserae/texts/la')\n",
    "path = expanduser(relativepath)\n",
    "onlyfiles = [f for f in listdir(path) if isfile(join(path, f)) and 'augustine' not in f and 'ambrose' not in f and 'jerome' not in f and 'tertullian' not in f and 'eugippius' not in f and 'hilary' not in f]\n",
    "onlyfiles = [join(path, f) for f in onlyfiles]\n",
    "COUNT_LIBRARY = dict()\n",
    "for filename in onlyfiles:\n",
    "    print(filename)\n",
    "    if '.tess' in filename:\n",
    "        read_files(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the COUNT_LIBRARY data structure has been built, it's time to test its ability to assign a probability distribution to all possible lemmas in the ambiguous context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_count(target, control = 0):\n",
    "    '''Assigns probability values to all possible lemmas.\n",
    "    parameters\n",
    "    ----------\n",
    "    target: the word being lemmatized\n",
    "    control: options for testing. 0 = use frequency values; 1 = choose at random; 2 = take the first option\n",
    "    sample output\n",
    "    -------------\n",
    "    [(lemma1, .45), (lemma2, .55)]\n",
    "    '''\n",
    "    lemmas = lemmatizer.lookup([target])\n",
    "    lemmas = lemmatizer.isolate(lemmas)\n",
    "    if len(lemmas) > 1:\n",
    "        if control == 1:\n",
    "            lemmalist = []\n",
    "            choice = randint(0, (len(lemmas) - 1))\n",
    "            lemmaobj = (lemmas[choice], 1)\n",
    "            lemmalist.append(lemmaobj)\n",
    "            return lemmalist\n",
    "        #this will return an even distribution, which will result in the 1st result being picked.\n",
    "        if control == 2:\n",
    "            lemmalist = lemmatizer.lookup([target])\n",
    "            lemmalist = lemmalist[0][1]\n",
    "            return lemmalist        \n",
    "        if control == 3:\n",
    "            all_lemmas_total = sum([COUNT_LIBRARY[lem] for lem in lemmas])\n",
    "            # the probability distribution is just the # of appearances in the corpus for one lemma\n",
    "            # vs. the number of appearances for all lemmata, total.\n",
    "            return ([(lem, (COUNT_LIBRARY[lem] / all_lemmas_total)) for lem in lemmas])\n",
    "    else:\n",
    "        lemmalist = []\n",
    "        lemmaobj = (lemmas[0], 1)\n",
    "        lemmalist.append(lemmaobj)\n",
    "        return lemmalist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a test of the lemmatizer on the first 1000 sentences of parsed Latin in the latin_cltk_data repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessobj = TessFile(onlyfiles[389])\n",
    "tokengenerator = iter(tessobj.read_tokens())\n",
    "tokens = new_file(tokengenerator, 4)\n",
    "target = tokens.pop(1)\n",
    "compare_context(target, tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
