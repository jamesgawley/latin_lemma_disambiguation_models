{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is designed to scan the entire Tesserae text folder and build contextual data\n",
    "for each type in the corpus. This is necessary for certain experiments involving the ideal\n",
    "lemma (in ambiguous cases), synonym, or translation for a given token-in-context during\n",
    "NLP tasks. Contributors: James Gawley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, expanduser\n",
    "from collections import defaultdict\n",
    "from pprint import PrettyPrinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/James/cltk')\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.semantics.latin.lookup import Lemmata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/James/tesserae-v5')\n",
    "from tesserae.utils import TessFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKIP_LIBRARY is a large dictionary whose keys are lemmas and whose values are dictionaries.\n",
    "Second-layer dictionaries describe context of word forms in corpus through counts of \n",
    "surrounding word-forms.\n",
    "\n",
    "In the original version of this data structure, the SKIP_LIBRARY keys were normalized tokens–in other words, inflected forms of Latin words. Problematically, converting from tokens to lemmas *massively* increased run time. Instead of several hours, the code would take several weeks to execute. The only changes were to the skipgram() method. Specifically, these lines were added:\n",
    "\n",
    "    lemmatizer = Lemmata(dictionary = 'lemmata', language = 'latin')\n",
    "    lemmas = lemmatizer.lookup(targettoken)\n",
    "    lemmas = lemmatizer.isolate(lemmas)\n",
    "\n",
    "One possible problem is that a new lemmatizer is instantiated at each step in the program's main loop–in other words, close to 10 million times. Since it's the same lemmatizer, this doesn't actually need to happen. The problem is python's (lack of) scope.\n",
    "\n",
    "The idea behind making lemmas the SKIP_LIBRARY keys is that the contextual information for each lemma will be drawn from the inflected form in the corpus. When a form is ambiguous, contextual info for both possible lemmas are updated. When the form is unambiguous, only the correct lemma is updated. So when we lemmatize in-context, we can look at the surrounding word forms and compare that context to the stored context for each lemma. If the token is ambiguous but it's surrounding words look like the words we saw in unambiguous cases, then we know which possible lemma is more likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_LIBRARY = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filepath, context_window):\n",
    "    '''Moves through a .tess file and calls the 'next' and 'skipgram' functions as needed.\n",
    "    Updates the SKIP_LIBRARY global object.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath: a file in .tess format\n",
    "    context_window: how many words on either side of the target to look at.\n",
    "    '''\n",
    "    tessobj = TessFile(filepath)\n",
    "    tokengenerator = iter(tessobj.read_tokens())\n",
    "    tokens = new_file(tokengenerator, context_window)\n",
    "    stop = 0\n",
    "    while stop != 1:\n",
    "        #the target should be five away from the end of the file, until the end\n",
    "        target_position = len(tokens) - (context_window + 1)\n",
    "        targettoken = tokens[target_position]\n",
    "        #grab all the other tokens but the target\n",
    "        contexttokens = [x for i, x in enumerate(tokens) if i != target_position]\n",
    "        #add this context to the skipgram map\n",
    "        skipgram(targettoken, contexttokens)\n",
    "        #prep the next token in the file\n",
    "        try:\n",
    "            rawtoken = next(tokengenerator)\n",
    "            cleantoken = token_cleanup(rawtoken)            \n",
    "            tokens.append(cleantoken)\n",
    "            if len(tokens) > (context_window * 2 + 1):\n",
    "                tokens.pop(0)\n",
    "        except StopIteration:\n",
    "            #we have reached EOF. Loop through until the last token is done then quit\n",
    "            #when this happens, the token list should have 11 indices, and the 'target_position'\n",
    "            #index will be the sixth (i.e. :tokens[5]). Pop the first index off, leaving 10\n",
    "            #indices and making the sixth index (previously the seventh) the new target.\n",
    "            while len(tokens) > (context_window):\n",
    "                tokens.pop(0)\n",
    "                # This loop makes the target_position move to the end. E.g. if the context_window is 6, then\n",
    "                # as long as there are six or more indexes, make the target_position the sixth index.\n",
    "                if len(tokens) > (context_window + 1):\n",
    "                    target_position = (context_window)\n",
    "                # But if there six or fewer indexes, then the target_position is the last index.\n",
    "                else:\n",
    "                    target_position = len(tokens) - 1\n",
    "                targettoken = tokens[target_position]\n",
    "                #grab all the other tokens but the target\n",
    "                contexttokens = [x for i, x in enumerate(tokens) if i != target_position]\n",
    "                #add this context to the skipgram map\n",
    "                skipgram(targettoken, contexttokens)\n",
    "            stop = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(targettoken, contexttokens):\n",
    "    '''Builds a complex data structure that will contain the 'average context'\n",
    "    for each type in the corpus. Updates SKIP_LIBRARY.\n",
    "    param targettoken: the token in question\n",
    "    param contexttokens: list of tokens surrounding the targettoken\n",
    "    global SKIP_LIBRARY: a dictionary whose keys are types and whose values are\n",
    "    dictionaries; see above.\n",
    "    '''\n",
    "    global SKIP_LIBRARY\n",
    "    lemmatizer = Lemmata(dictionary = 'lemmata', language = 'latin')\n",
    "    lemmas = lemmatizer.lookup(targettoken)\n",
    "    lemmas = lemmatizer.isolate(lemmas)\n",
    "    for lemma in lemmas:\n",
    "        if lemma not in SKIP_LIBRARY:\n",
    "            SKIP_LIBRARY[lemma] = defaultdict(int)\n",
    "        for contextword in contexttokens:\n",
    "            SKIP_LIBRARY[lemma][contextword] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_file(tokengenerator, context_window):\n",
    "    '''Takes an iterator object for the file being read.\n",
    "    Reads in the first n tokens and returns them'''\n",
    "    tokens = []\n",
    "    for i in range(0, (context_window + 1)):\n",
    "        rawtoken = next(tokengenerator)\n",
    "        cleantoken = token_cleanup(rawtoken)\n",
    "        # NB: right now the code assumes that first sentence is > n + 1 words\n",
    "        tokens.append(cleantoken)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_cleanup(rawtoken):\n",
    "    '''This method is intented to make word-forms in the corpus more uniform.'''\n",
    "    jv = JVReplacer()\n",
    "    word_tokenizer = WordTokenizer('latin')\n",
    "    rawtoken = jv.replace(rawtoken)\n",
    "    rawtoken = rawtoken.lower()\n",
    "    tokenlist = word_tokenizer.tokenize(rawtoken)\n",
    "    return tokenlist[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the actual program loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open all the tesserae files\n",
    "relativepath = join('~/cleantess/tesserae/texts/la')\n",
    "path = expanduser(relativepath)\n",
    "onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "onlyfiles = [join(path, f) for f in onlyfiles]\n",
    "\n",
    "for filename in onlyfiles:\n",
    "    print(filename)\n",
    "    if '.tess' in filename:\n",
    "        read_files(filename, context_window = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
