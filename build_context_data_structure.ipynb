{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is designed to scan the entire Tesserae text folder and build contextual data\n",
    "for each type in the corpus. This is necessary for certain experiments involving the ideal\n",
    "lemma (in ambiguous cases), synonym, or translation for a given token-in-context during\n",
    "NLP tasks. Contributors: James Gawley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, expanduser\n",
    "from collections import defaultdict\n",
    "from pprint import PrettyPrinter\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from operator import itemgetter\n",
    "from difflib import SequenceMatcher\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = os.path.join('~/cltk')\n",
    "path = os.path.expanduser(rel_path)\n",
    "os.chdir(path)\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.semantics.latin.lookup import Lemmata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This filepath needs to be customized. The git repo is located at https://github.com/jeffkinnison/tesserae-v5.git\n",
    "os.chdir('/Users/James/tesserae-v5')\n",
    "from tesserae.utils import TessFile\n",
    "from tesserae.utils import TessFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKIP_LIBRARY is a large dictionary whose keys are lemmas and whose values are dictionaries.\n",
    "Second-layer dictionaries describe context of word forms in corpus through counts of \n",
    "surrounding word-forms.\n",
    "\n",
    "In the original version of this data structure, the SKIP_LIBRARY keys were normalized tokens–in other words, inflected forms of Latin words. Problematically, converting from tokens to lemmas *massively* increased run time. Instead of several hours, the code would take several weeks to execute. The only changes were to the skipgram() method. Specifically, these lines were added:\n",
    "\n",
    "    lemmatizer = Lemmata(dictionary = 'lemmata', language = 'latin')\n",
    "    lemmas = lemmatizer.lookup(targettoken)\n",
    "    lemmas = lemmatizer.isolate(lemmas)\n",
    "\n",
    "One possible problem is that a new lemmatizer is instantiated at each step in the program's main loop–in other words, close to 10 million times. Since it's the same lemmatizer, this doesn't actually need to happen. The problem is python's (lack of) scope.\n",
    "\n",
    "The idea behind making lemmas the SKIP_LIBRARY keys is that the contextual information for each lemma will be drawn from the inflected form in the corpus. When a form is ambiguous, contextual info for both possible lemmas are updated. When the form is unambiguous, only the correct lemma is updated. So when we lemmatize in-context, we can look at the surrounding word forms and compare that context to the stored context for each lemma. If the token is ambiguous but it's surrounding words look like the words we saw in unambiguous cases, then we know which possible lemma is more likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_LIBRARY = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filepath, context_window):\n",
    "    '''Moves through a .tess file and calls the 'next' and 'skipgram' functions as needed.\n",
    "    Updates the SKIP_LIBRARY global object.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath: a file in .tess format\n",
    "    context_window: how many words on either side of the target to look at.\n",
    "    '''\n",
    "    tessobj = TessFile(filepath)\n",
    "    tokengenerator = iter(tessobj.read_tokens())\n",
    "    tokens = new_file(tokengenerator, context_window)\n",
    "    stop = 0\n",
    "    while stop != 1:\n",
    "        #the target should be five away from the end of the file, until the end\n",
    "        target_position = len(tokens) - (context_window + 1)\n",
    "        targettoken = tokens[target_position]\n",
    "        #grab all the other tokens but the target\n",
    "        contexttokens = [x for i, x in enumerate(tokens) if i != target_position]\n",
    "        #add this context to the skipgram map\n",
    "        skipgram(targettoken, contexttokens)\n",
    "        #prep the next token in the file\n",
    "        try:\n",
    "            rawtoken = next(tokengenerator)\n",
    "            cleantoken = token_cleanup(rawtoken)            \n",
    "            tokens.append(cleantoken)\n",
    "            if len(tokens) > (context_window * 2 + 1):\n",
    "                tokens.pop(0)\n",
    "        except StopIteration:\n",
    "            #we have reached EOF. Loop through until the last token is done then quit\n",
    "            #when this happens, the token list should have n * 2 + 1 indices, and the 'target_position'\n",
    "            #index will be n + 1. Pop the first index off, leaving n * 2. The target will be \n",
    "            #just past halfway through the list. Keep popping until target reaches end of list.\n",
    "            while len(tokens) > (context_window):\n",
    "                tokens.pop(0)\n",
    "                # This loop makes the target_position move to the end. E.g. if the context_window is 6, then\n",
    "                # as long as there are six or more indexes, make the target_position the sixth index.\n",
    "                if len(tokens) > (context_window + 1):\n",
    "                    target_position = (context_window)\n",
    "                # But if there six or fewer indexes, then the target_position is the last index.\n",
    "                else:\n",
    "                    target_position = len(tokens) - 1\n",
    "                targettoken = tokens[target_position]\n",
    "                #grab all the other tokens but the target\n",
    "                contexttokens = [x for i, x in enumerate(tokens) if i != target_position]\n",
    "                #add this context to the skipgram map\n",
    "                skipgram(targettoken, contexttokens)\n",
    "            stop = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = Lemmata(dictionary = 'lemmata', language = 'latin')\n",
    "def skipgram(targettoken, contexttokens):\n",
    "    '''Builds a complex data structure that will contain the 'average context'\n",
    "    for each type in the corpus. Updates SKIP_LIBRARY.\n",
    "    param targettoken: the token in question\n",
    "    param contexttokens: list of tokens surrounding the targettoken\n",
    "    global SKIP_LIBRARY: a dictionary whose keys are types and whose values are\n",
    "    dictionaries; see above.\n",
    "    '''\n",
    "    global SKIP_LIBRARY\n",
    "    lemmas = lemmatizer.lookup(targettoken)\n",
    "    lemmas = lemmatizer.isolate(lemmas)\n",
    "    for lemma in lemmas:\n",
    "        if lemma not in SKIP_LIBRARY:\n",
    "            SKIP_LIBRARY[lemma] = defaultdict(int)\n",
    "        for contextword in contexttokens:\n",
    "            SKIP_LIBRARY[lemma][contextword] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_file(tokengenerator, context_window):\n",
    "    '''Takes an iterator object for the file being read.\n",
    "    Reads in the first n tokens and returns them'''\n",
    "    tokens = []\n",
    "    for i in range(0, (context_window + 1)):\n",
    "        rawtoken = next(tokengenerator)\n",
    "        cleantoken = token_cleanup(rawtoken)\n",
    "        # NB: right now the code assumes that first sentence is > n + 1 words\n",
    "        tokens.append(cleantoken)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "jv = JVReplacer()\n",
    "word_tokenizer = WordTokenizer('latin')\n",
    "def token_cleanup(rawtoken):\n",
    "    '''This method is intented to make word-forms in the corpus more uniform.'''\n",
    "    rawtoken = jv.replace(rawtoken)\n",
    "    rawtoken = rawtoken.lower()\n",
    "    tokenlist = word_tokenizer.tokenize(rawtoken)\n",
    "    return tokenlist[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the actual program loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open all the tesserae files\n",
    "relativepath = join('~/cleantess/tesserae/texts/la')\n",
    "path = expanduser(relativepath)\n",
    "onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "onlyfiles = [join(path, f) for f in onlyfiles]\n",
    "\n",
    "for filename in onlyfiles:\n",
    "    print(filename)\n",
    "    if '.tess' in filename:\n",
    "        read_files(filename, context_window = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the SKIP_LIBRARY data structure has been built, it's time to test its ability to assign a probability distribution to all possible lemmas in the ambiguous context. This can be done by comparing the context in which the target token is found against the representative context in SKIP_LIBRARY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_context(target, context):\n",
    "    '''Assigns a probability value to each possible lemma in ambiguous context.\n",
    "    returns a standard cltk.semantics object.\n",
    "    params\n",
    "    ------\n",
    "    target: the token to lemmatize\n",
    "    context: tokens found in the vicinity of the target, in situ\n",
    "    '''\n",
    "    #gather a list of possible lemmas\n",
    "    lemmas = lemmatizer.lookup(target)\n",
    "    lemmas = lemmatizer.isolate(lemmas)\n",
    "    #if there is more than one possibility, load up their lemma-contexts from SKIP_LIBRARY\n",
    "    if len(lemmas) > 1:\n",
    "        shared_context_counts = dict()\n",
    "        for lem in lemmas:\n",
    "            #the number of context words in common will always be even, between lemmas, \n",
    "            #unless SKIP_LIBRARY is trained on a different corpus than the one being lemmatized.\n",
    "            #so instead of *whether* a word was seen, we rely on how many times.\n",
    "            lemma_context_dictionary = SKIP_LIBRARY[lem]\n",
    "            lemma_context_words = lemma_context_dictionary.keys()\n",
    "            counts = [lemma_context_dictionary[context_token] for context_token in set(context).intersection(lemma_context_words)]\n",
    "            shared_context_counts[lem] = sum(counts)\n",
    "            print(shared_context_counts[lem])\n",
    "        total_shared = sum(shared_context_counts.values())\n",
    "        lemmalist = []\n",
    "        for lem in lemmas:\n",
    "            lemmaprob = shared_context_counts[lem] / total_shared\n",
    "            lemmaobj = (lem, lemmaprob)\n",
    "            lemmalist.append(lemmaobj)\n",
    "        return lemmalist\n",
    "    else:\n",
    "        return lemmatizer.lookup(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a test of the lemmatizer on the first words of the Aeneid. It happens that the Aeneid is file number 389 in my folder; this will not be true on all installs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessobj = TessFile(onlyfiles[389])\n",
    "tokengenerator = iter(tessobj.read_tokens())\n",
    "tokens = new_file(tokengenerator, 4)\n",
    "target = tokens.pop(1)\n",
    "compare_context(target, tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
